{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.9.6 64-bit ('kaggle': conda)"},"language_info":{"name":"python","version":"3.9.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"96569ffd5c0ab42d85a1497b3ee1b642c42bb3cbc1c7ced658b6488ac1cb84ae"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["Welcome to the **[30 Days of ML competition](https://www.kaggle.com/c/30-days-of-ml/overview)**!  In this notebook, you'll learn how to make your first submission.\n","\n","Before getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n","\n","# Step 1: Import helpful libraries\n","\n","We begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)** course."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Familiar imports\n","import numpy as np\n","import pandas as pd\n","\n","# For ordinal encoding categorical variables, splitting data\n","from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","\n","# For training random forest model\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","#for the xgboost \n","import xgboost \n","from sklearn.model_selection import GridSearchCV"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:16:59.388155Z","iopub.execute_input":"2021-08-19T12:16:59.388607Z","iopub.status.idle":"2021-08-19T12:16:59.394781Z","shell.execute_reply.started":"2021-08-19T12:16:59.388573Z","shell.execute_reply":"2021-08-19T12:16:59.393479Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["#we will set some variables\n","JOBS=4 #for the number of jobs in the ml algorithm"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["# Step 2: Load the data\n","\n","Next, we'll load the training and test data.  \n","\n","We set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Load the training data\n","train = pd.read_csv(\"../input/30-days-of-ml/train.csv\", index_col=0)\n","test = pd.read_csv(\"../input/30-days-of-ml/test.csv\", index_col=0)\n","\n","# Preview the data that will be used for training\n","train.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:17:04.166395Z","iopub.execute_input":"2021-08-19T12:17:04.166753Z","iopub.status.idle":"2021-08-19T12:17:06.732186Z","shell.execute_reply.started":"2021-08-19T12:17:04.166723Z","shell.execute_reply":"2021-08-19T12:17:06.730679Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["#preview of the test data\n","test.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:17:16.642353Z","iopub.execute_input":"2021-08-19T12:17:16.642903Z","iopub.status.idle":"2021-08-19T12:17:16.672168Z","shell.execute_reply.started":"2021-08-19T12:17:16.642867Z","shell.execute_reply":"2021-08-19T12:17:16.670891Z"},"trusted":true}},{"cell_type":"markdown","source":["The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`)."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Separate target from features\n","y = train['target']\n","features = train.drop(['target'], axis=1)\n","\n","# Preview features\n","features.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:17:40.804661Z","iopub.execute_input":"2021-08-19T12:17:40.805123Z","iopub.status.idle":"2021-08-19T12:17:40.891308Z","shell.execute_reply.started":"2021-08-19T12:17:40.805086Z","shell.execute_reply":"2021-08-19T12:17:40.889882Z"},"trusted":true}},{"cell_type":"markdown","source":["# Step 3: Prepare the data\n","\n","Next, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n","\n","In the **[Categorical Variables lesson](https://www.kaggle.com/alexisbcook/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# List of categorical columns\n","object_cols = [col for col in features.columns if 'cat' in col]\n","\n","# get the numeric columns also\n","num_cols = [col for col in features.columns if 'cat' not in col]\n","\n","#check the categorical column's cardinality\n","col_cardinality = dict.fromkeys(object_cols, None)\n","for key in col_cardinality.keys():\n","    col_cardinality[key] = features[key].nunique()\n","print(\"each column and it's cardinality {}\".format(col_cardinality))\n","\n","cardinalities = set(col_cardinality.values())\n","print(\"the distinct cardinality {}\".format(cardinalities))\n"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:09:52.419704Z","iopub.execute_input":"2021-08-19T12:09:52.420079Z","iopub.status.idle":"2021-08-19T12:09:52.939953Z","shell.execute_reply.started":"2021-08-19T12:09:52.420048Z","shell.execute_reply":"2021-08-19T12:09:52.938409Z"},"trusted":true}},{"cell_type":"markdown","source":["We will loop all over the possible values of the cardinality, to check which one will give us the better result, regarding the chosen metric (mean squared error)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["def get_input(col_cardinality, cardinality, num_cols):   \n","    '''\n","    Helper function. Returns the X and X_test datasets for a specific value for cardinality\n","    '''\n","    # we are gonna pick the low cardinality cols [arbitary to 4] and apply one hot encoding\n","    sel_obj_cols = [key for key in col_cardinality.keys() if col_cardinality[key]<=cardinality]\n","    #print(sel_obj_cols)\n","\n","    #get the final columns\n","    final_cols = sel_obj_cols + num_cols\n","    #print(final_cols)\n","\n","    # select final columns\n","    subfeatures = features[final_cols]\n","    subtest = test[final_cols]\n","\n","    #we will use One Hot Encoder for the categorical data\n","    ohe_encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n","    OH_cols_train = pd.DataFrame(ohe_encoder.fit_transform(subfeatures[sel_obj_cols]))\n","    OH_cols_valid = pd.DataFrame(ohe_encoder.transform(subtest[sel_obj_cols]))\n","\n","    # One-hot encoding removed index; put it back\n","    OH_cols_train.index = subfeatures.index\n","    OH_cols_valid.index = subtest.index\n","\n","    # Remove categorical columns (will replace with one-hot encoding)\n","    num_X_train = subfeatures.drop(sel_obj_cols, axis=1)\n","    num_X_valid = subtest.drop(sel_obj_cols, axis=1)\n","\n","    # Add one-hot encoded columns to numerical features\n","    X = pd.concat([num_X_train, OH_cols_train], axis=1)\n","    X_test = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n","    return X, X_test\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["def fit_model(col_cardinality, cardinality, num_cols):\n","    '''\n","    Helper function, returns the model and rmse on a dataset defined for a specific cardinality value\n","    '''\n","    #to get the X and X_test datasets we are gonna utilize the previous function\n","    X, X_test = get_input(col_cardinality, cardinality, num_cols)\n","    #preview of the data\n","    #X.head()\n","\n","    #create the test and training sets [split 0.80-0.20]\n","    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, train_size=0.80, random_state=0)\n","\n","    # Define the Random Forest model [set parametres]\n","    model = RandomForestRegressor(n_estimators=150, max_depth=10, n_jobs=JOBS, random_state=1)\n","\n","    # Train the model\n","    model.fit(X_train, y_train)\n","    preds_valid = model.predict(X_valid)\n","    rmse = (mean_squared_error(y_valid, preds_valid, squared=False))\n","    return model, rmse"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["models = [] #placeholder for the different models\n","rmses = [] #placeholder for the different root mean squared errors calculated \n","#loop over the different value of cardinalities\n","for cardinality in cardinalities:\n","    model, rmse = fit_model(col_cardinality, cardinality, num_cols)\n","    rmses.append(rmse)\n","    models.append(model)\n","    print(\"for cardinality {c}, the root mean squared error is {r}\".format(c=cardinality, r=rmses[-1]))"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-19T12:23:01.860639Z","iopub.execute_input":"2021-08-19T12:23:01.861085Z","iopub.status.idle":"2021-08-19T12:23:01.996837Z","shell.execute_reply.started":"2021-08-19T12:23:01.861039Z","shell.execute_reply":"2021-08-19T12:23:01.995593Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["#get the min value of the rmses and the cardinality\n","print(\"min value of the rmes's is : {}\".format(min(rmses)))\n","best_cardinality = list(cardinalities)[rmses.index(min(rmses))]\n","print(\"the corresponding cardinality is : {}\".format(best_cardinality))\n","\n","#setting the best model \n","#best_model = models[rmses.index(min(rmses))]\n","\n","#set the input for the best cardinality\n","X, X_test = get_input(col_cardinality, best_cardinality, num_cols)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["#initiliaze the model with basic parametres\n","#xgb_model = xgb.XGBRegressor(n_jobs=JOBS, random_state=1, n_estimators=2000, max_depth=15, learning_rate=0.1)\n","xgb_model = xgb.XGBRegressor(n_jobs=JOBS, random_state=1)\n","\n","#specify the some parametres to optimize\n","params = {\n","    'n_estimators' : [1000, 1500, 2000],\n","    'max_depth' : [10, 12],\n","    'subsample' : [0.8, 0.9],\n","    'learning_rate' : [0.05, 0.1, 0.2]\n","}\n","\n","grid_search = GridSearchCV(xgb_model, params, cv=5, n_jobs=JOBS, scoring='accuracy') #neg_root_mean_squared_error\n","\n","#create the test and training sets [split 0.80-0.20]\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, train_size=0.80, random_state=0)\n","\n","#preview the first rows of the training set\n","X_train.head()"],"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'xgboost'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_44289/3077654033.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#initiliaze the model with basic parametres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#xgb_model = xgb.XGBRegressor(n_jobs=JOBS, random_state=1, n_estimators=2000, max_depth=15, learning_rate=0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"]}],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["grid_search.fit(X_train, y_train)\n","\n","#define the best model\n","best_model = grid_search.best_estimator_"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["#get the best parameters that were evaluated\n","best_model_params = grid_search.best_params_"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["#calculate the rmse for the best evaluated model\n","preds_valid = best_model.predict(X_valid)\n","rmse = (mean_squared_error(y_valid, preds_valid, squared=False))\n","\n","print('rmse : {}'.format(rmse))"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n","\n","# Step 5: Submit to the competition\n","\n","We'll begin by using the trained model to generate predictions, which we'll save to a CSV file."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Use the model to generate predictions\n","predictions = best_model.predict(X_test)\n","\n","# Save the predictions to a CSV file\n","output = pd.DataFrame({'Id': X_test.index,\n","                       'target': predictions})\n","output.to_csv('submission.csv', index=False)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["Once you have run the code cell above, follow the instructions below to submit to the competition:\n","1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n","2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n","3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n","4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n","\n","You have now successfully submitted to the competition!\n","\n","If you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work."],"metadata":{}},{"cell_type":"markdown","source":["# Step 6: Keep Learning!\n","\n","If you're not sure what to do next, you can begin by trying out more model types!\n","1. If you took the **[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)** course, then you learned about **[XGBoost](https://www.kaggle.com/alexisbcook/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n","\n","2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https://www.kaggle.com/svyatoslavsokolov/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset."],"metadata":{}}]}